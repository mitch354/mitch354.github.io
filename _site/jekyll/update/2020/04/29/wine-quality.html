<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Applying Machine Learning Techniques on Wine Rankings | Mitchell Hole</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Applying Machine Learning Techniques on Wine Rankings" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Goal" />
<meta property="og:description" content="Goal" />
<link rel="canonical" href="http://localhost:4000/jekyll/update/2020/04/29/wine-quality.html" />
<meta property="og:url" content="http://localhost:4000/jekyll/update/2020/04/29/wine-quality.html" />
<meta property="og:site_name" content="Mitchell Hole" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-04-29T07:43:49-07:00" />
<script type="application/ld+json">
{"description":"Goal","@type":"BlogPosting","url":"http://localhost:4000/jekyll/update/2020/04/29/wine-quality.html","headline":"Applying Machine Learning Techniques on Wine Rankings","dateModified":"2020-04-29T07:43:49-07:00","datePublished":"2020-04-29T07:43:49-07:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/jekyll/update/2020/04/29/wine-quality.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Mitchell Hole" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Mitchell Hole</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a><a class="page-link" href="/education/">Education</a><a class="page-link" href="/Projects/">Projects</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Applying Machine Learning Techniques on Wine Rankings</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2020-04-29T07:43:49-07:00" itemprop="datePublished">Apr 29, 2020
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h3 id="goal">Goal</h3>

<p>Look for a relationship between chemical properties of wine and their quality rating from experts. Will explore several different models (linear regression, logistic regression &amp; Neural Networks) to try to find an accurate predictor.</p>

<h3 id="data-collection">Data Collection</h3>

<p>Data imported from:</p>

<p>P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis.
Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009.</p>

<p>Obtained a CSV file of 1600 samples of wine. Features are:</p>

<p><code class="highlighter-rouge">"fixed acidity"</code></p>

<p><code class="highlighter-rouge">"volatile acidity"</code></p>

<p><code class="highlighter-rouge">"citric acid"</code></p>

<p><code class="highlighter-rouge">"residual sugar"</code></p>

<p><code class="highlighter-rouge">"chlorides"</code></p>

<p><code class="highlighter-rouge">"free sulfur dioxide"</code></p>

<p><code class="highlighter-rouge">"total sulfur dioxide"</code></p>

<p><code class="highlighter-rouge">"density"</code></p>

<p><code class="highlighter-rouge">"pH"</code></p>

<p><code class="highlighter-rouge">"sulphates"</code></p>

<p><code class="highlighter-rouge">"alcohol"</code></p>

<p><code class="highlighter-rouge">"quality"</code> - a whole number between 1 &amp; 10, is the output variable and what I will be trying to predict as an function of the other feautures.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">redWines</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'winequality-red.csv'</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s">";"</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">redWines</span><span class="p">[</span><span class="s">'quality'</span><span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">redWines</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s">'quality'</span><span class="p">])</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="data-engineering">Data Engineering</h3>

<p>Since several features were on different scales I applied mean normalization:</p>

<p><code class="highlighter-rouge">X=(X-X.mean())/X.std()</code></p>

<p>Some features also show a great deal of correlation:</p>

<p><img src="/imgs/2020-04-30/heatmap.png" title="heatmap" /></p>

<p>After confirming they had no impact on model accuracy, citric acid and and fixed acidity were dropped. PH sufficiently gave the same information.</p>

<p>I considered applying PCA but it lead to a significant loss of information:</p>

<p><em>Components = 2</em></p>

<p><img src="/imgs/2020-04-30/2pca.png" title="2pca" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span>
<span class="n">array</span><span class="p">([</span><span class="mf">0.23738404</span><span class="p">,</span> <span class="mf">0.18819661</span><span class="p">])</span>
</code></pre></div></div>

<p><em>Components = 3</em></p>

<p><img src="/imgs/2020-04-30/3pca.png" title="3pca" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span>
<span class="n">array</span><span class="p">([</span><span class="mf">0.23738404</span><span class="p">,</span> <span class="mf">0.18819661</span><span class="p">,</span> <span class="mf">0.15882354</span><span class="p">])</span>
</code></pre></div></div>

<h3 id="results">Results</h3>

<p><strong>Linear Regression</strong></p>

<p>Training/Test split:</p>

<p><code class="highlighter-rouge">X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)</code></p>

<p>Model:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span>
    <span class="n">LinearRegression</span><span class="p">()</span>
<span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
</code></pre></div></div>

<p>Score:</p>

<p>Runnning with 100 different test/train splits average scores were:</p>

<p><strong><em>Red Wine Training Score: 0.361</em></strong></p>

<p><strong><em>Red Wine Validation Score: 0.348</em></strong></p>

<p>The scores here are the R2 coefficient and essentially shows how correlated ((y_true - y_pred) ** 2).sum() and the residual sum of squares ((y_true - y_true.mean()) ** 2).sum() are. ~0.27 - 0.35 are pretty weak correlations. The training score and validation score are very close though so at least the model is generalizing well and not biased.</p>

<p>I also calculated the mean-squared error to get a better sense of how far away my predictions were:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_predicted</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">regression_model_mse</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_predicted</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="n">regression_model_mse</span>
<span class="k">print</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">regression_model_mse</span><span class="p">))</span>
</code></pre></div></div>

<p>This gave an average error in quality rating around:</p>

<p><strong><em>Red Wine MSE: 0.65</em></strong></p>

<p>The data proved to be not very polynomial as adding these features quickly led to poor validation scores:</p>

<pre><code class="language-Python">model = make_pipeline(
    PolynomialFeatures(degree=2),
    LinearRegression()
)
</code></pre>

<p>Adding logarithmic transformation also had no impact on scoring.</p>

<p>The main issue is the output variable, quality. It is essentially on a discrete scale since quality ratings are given as a whole number in range 1 - 10 (only 3 - 8 are actually assigned in the dataset).</p>

<p><img src="/imgs/2020-04-30/scatter.png" title="scatter" /></p>

<p>With this in mind linear regression may have not been the right tool for this job.</p>

<p><strong>Logistic Regression</strong></p>

<p>With the all the quality ratings belonging to [3,4,5,6,7,8] classification seems like an appropriate choice for this dataset.</p>

<table>
  <thead>
    <tr>
      <th>Quality</th>
      <th>Frequency</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>5</td>
      <td>0.425891</td>
    </tr>
    <tr>
      <td>6</td>
      <td>0.398999</td>
    </tr>
    <tr>
      <td>7</td>
      <td>0.124453</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.033146</td>
    </tr>
    <tr>
      <td>8</td>
      <td>0.011257</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.006254</td>
    </tr>
  </tbody>
</table>

<p>Like with linear regression the data was underfitting the test set</p>

<pre><code class="language-Python">model = make_pipeline(
    LogisticRegression(max_iter=250)
)
</code></pre>

<p><strong><em>Red Wine Training Score: 0.567</em></strong></p>

<p><strong><em>Red Wine Validation Score: 0.562</em></strong></p>

<p>(The score here is the ratio of correctly predicted classes)</p>

<pre><code class="language-Python">model = make_pipeline(
    LogisticRegression(C=1e4, max_iter=250)
)
</code></pre>

<p>To tackle this I raised the regularization parameter and that lead to, marginally, better performance:</p>

<p><strong><em>Red Wine Training Score: 0.596</em></strong></p>

<p><strong><em>Red Wine Validation Score: 0.592</em></strong></p>

<p>This is certainly better than chance and predictions are usually off by just a single rating:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">confusion_matrix2</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_train</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">y_predicted</span><span class="p">,</span><span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">8</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="n">confusion_matrix2</span><span class="p">)</span>
    <span class="mi">3</span>    <span class="mi">4</span>   <span class="mi">5</span>   <span class="mi">6</span>   <span class="mi">7</span>   <span class="mi">8</span>
<span class="mi">3</span><span class="p">[[</span>  <span class="mi">0</span>   <span class="mi">0</span>   <span class="mi">7</span>   <span class="mi">0</span>   <span class="mi">0</span>   <span class="mi">0</span><span class="p">]</span>
 <span class="mi">4</span><span class="p">[</span>  <span class="mi">0</span>   <span class="mi">0</span>  <span class="mi">29</span>  <span class="mi">16</span>   <span class="mi">2</span>   <span class="mi">0</span><span class="p">]</span>
 <span class="mi">5</span><span class="p">[</span>  <span class="mi">0</span>   <span class="mi">0</span> <span class="mi">408</span> <span class="mi">122</span>   <span class="mi">2</span>   <span class="mi">0</span><span class="p">]</span>
 <span class="mi">6</span><span class="p">[</span>  <span class="mi">0</span>   <span class="mi">0</span> <span class="mi">171</span> <span class="mi">328</span>  <span class="mi">16</span>   <span class="mi">0</span><span class="p">]</span>
 <span class="mi">7</span><span class="p">[</span>  <span class="mi">0</span>   <span class="mi">0</span>   <span class="mi">9</span> <span class="mi">126</span>  <span class="mi">26</span>   <span class="mi">0</span><span class="p">]</span>
 <span class="mi">8</span><span class="p">[</span>  <span class="mi">0</span>   <span class="mi">0</span>   <span class="mi">0</span>   <span class="mi">9</span>   <span class="mi">8</span>   <span class="mi">0</span><span class="p">]]</span>

</code></pre></div></div>

<h3 id="conclusion">Conclusion</h3>

<p>Using Linear and Logistic regression lead to some significantly better than chance results but not quite what I was hoping for. While not written about here, other learning algorithms like Support Vector Classes and Neural Networks gave almost identical scores as Logistic Regression. I believe the issue lies in the nature of the target variable (wine quality). A 5/10 wine vs a 6/10 wine are fairly similar labels when it comes to classification; it’s not apples vs oranges. I will perform some error analysis and report on anything I find in the next post as well as build a classifier to differentiate red vs white wine; a task I expect the ML algorithms to perform much better on.</p>

  </div><a class="u-url" href="/jekyll/update/2020/04/29/wine-quality.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Mitchell Hole</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Mitchell Hole</li><li><a class="u-email" href="mailto:mhole@sfu.ca">mhole@sfu.ca</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/mitch354"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">mitch354</span></a></li><li><a href="https://www.linkedin.com/in/mitchell-hole-3646a9162"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#linkedin"></use></svg> <span class="username">mitchell-hole-3646a9162</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Projects &amp; Experience</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
